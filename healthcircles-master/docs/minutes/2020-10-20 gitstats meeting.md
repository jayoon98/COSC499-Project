Hi everyone,

Here's an update on how the meeting with Dr Hui about concerns about Gitstats and marking went.

Most importantly, Dr Hui recognizes that while we've all been getting used to Github and Gitstats, we may have not earned the marks we hoped for (especially with the improvements and bug fixes Brandon has been pushing out.) Because of this, she wants the class to come to a consensus between three options for the Gitstats marks up to now:
1. keep the marks as they are
2. waive the marks for this section of the course
3. remark the work that has been done so far using the newest version

There will be a poll for everyone to decide, and (according to university policy) we must reach a consensus to make a change to the syllabus.

I'm going to do my best to describe the other topics we discussed. These were more inconclusive, and we haven't reached concrete action items for them yet (we ran out of time).

### gitstats doesn't count design work, emails, Google docs etc
Yes, Dr Hui agrees this is a shortcoming with the tool. She had hoped that it would have integration with Google docs at least, but there was not enough time to develop this feature.  It doesn't fix counting LoC, but you can commit a line about whatever work you did (eg "discussed X with client; they think Y feature is more important than X") so you can at least get credit for the commit/issue/PR.

### about the TA/report marking
Currently Zil checks repos to look for clear instances of grade inflation (eg extra issues, generic comments that don't provide helpful feedback). If she doesn't find any problems, she enters our mark into Canvas according to the report we submit. A change was implemented in Gitstats a few weeks ago to allow effort scores to exceed 100% if you complete more than the 2 issues expected per week. This was intended to be a way to balance scores across weeks: if you have a week where you can't contribute as much, you can make it up in the next week. However, there was a miscommunication, and Zil has been capping our scores at 7/10 (so 10/10 including our marks from Friday's meeting) rather than allowing them to exceed 100%.

Zil has also been leaving comments on PRs and issues rather than emailing groups feedback through Canvas. The reason given for this is that the Github repo is where we're working, so feedback in this context makes the most sense and saves time. If you still feel strongly about this, I believe that Dr Hui is likely to acquiesce to this change.

### weekly group meetings
These are frustratingly short, but unfortunately that's a limitation we're dealing with because of covid this year. Dr Hui expressed that if we have questions about our project (such as parsing the expectations of the requirements report), then this would be a good time to ask them. 

### group evaluations on videos
We asked about dividing up review responsibilities for the next milestones and Dr Hui agreed. We suggested that each A group reviews other A groups etc, but Dr Hui also wants us to review the other groups completing the same project because we'll have a deeper understanding of the requirements. So peer testing should hopefully be a shorter process and yield more careful, considerate feedback.

### course marks in general
Dr Hui expressed that ultimately she wants our marks to reflect our actual understanding and progress, not end up with A's given out to people who have done the bare minimum or C's for people who went above and beyond. She wanted to reassure the class that last year, the lowest mark was like a 50 and the average was in the 80's. 

Some of the ways this class and others like it have been marked in the past include peer evaluations and logs of hourly time spent. With peer evaluation, she stated that marks are not honest (nearly everyone gives max marks) and she has faced issues where a group will highly rank a group member who is slacking off throughout the year, and then eventually come to her to discuss the problem. However, at that point the only existing data about work done is the peer evaluations, and it has become an issue of academic honesty that was taken to the dean in the past. She stated that the issue with time tracking is that many teams will just not submit their logs, so it is not a reliable way to mark.

She also explained that other institutions such as the University of Waterloo use similar automated grading systems to mark work, and while there is definitely difficulty at the beginning, students learn to work within the given constraints.

### grading with a threshold
We described the solution of setting a threshold of tasks to complete each week (whether it is the same for the entire course or set in each sprint planning session). Dr Hui asked where the threshold would be set and why; she also thinks that a threshold would result in easy 100%s for anyone doing the bare minimum work. For anyone who feels strongly about this, it may be helpful to find examples of other courses, projects, or workplaces where this method is used to justify the validity of it.
(I am writing all this on very little sleep and have not started my actual work for my team yet so I unfortunately do not have the time or energy to do so myself.)

### time spent running Gitstats
We told Dr Hui how many hours we spend on Gitstats (about 1-2 hours, plus all the dozen or so hours we've spent reading and re-reading documentation and code) and told her that other groups are facing similar demands on their time. She acknowledged that it was not intended to take up this much time, and was hoping that with the practice we've had so far it would be trivial at this point. So I wanted to ask all of you: are you still feeling lost using Gitstats or Github in general? Are there any particular things that you need more support to understand? For me personally, I still don't really understand how code reviews work, or what kind of comments you should give while writing them. (This probably should have been covered in 310; alas it was not.)

### creating issues for Gitstats
I wish we were instructed to do this from the start of the course, but if you run into any issues or bugs using Gitstats, then open on issue on the repo about it. Brandon has been really good about making changes quickly, or explaining why a requested change wouldn't work.

We mentioned to Dr Hui that a changelog of improvements would be helpful--for example, at one point issues created before the current week were not counted rendering our backlog useless; that's since been fixed-- but we've just been following the repo in the meantime.

### conclusion 
Ultimately, the point of Capstone is to judge whether we understand enough about computer science to earn our degree. With Gitstats, Dr Hui is attempting to judge not only if we know how to write code, but if we can work within constraints, communicate and define what we are working on, and of course use Github. If you feel that these learning outcomes are not being adequately measured and you have a better solution, please share your thoughts!



